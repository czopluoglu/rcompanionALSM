[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"R companion Applied Linear Statistical Models (Kutner et al. 2005). purpose book reproduce examples book using R provide solutions exercises using R.","code":""},{"path":"ch1.html","id":"ch1","chapter":"1 Linear Regression with One Predictor Variable","heading":"1 Linear Regression with One Predictor Variable","text":"","code":""},{"path":"ch1.html","id":"relations-between-variables","chapter":"1 Linear Regression with One Predictor Variable","heading":"1.1 Relations between Variables","text":"","code":""},{"path":"ch1.html","id":"example-1","chapter":"1 Linear Regression with One Predictor Variable","heading":"1.1.1 Example 1","text":"","code":"\nrequire(here)\n\nfig1.2 <- read.csv(here('/data/fig1.2.csv/'),header=FALSE)\n\nrequire(ggplot2)\n\nggplot(fig1.2,aes(x=V1,y=V2))+\n  geom_point(size=4,col='gray42')+\n  geom_smooth(method=lm,se=FALSE,lty=2,col='black')+\n  xlim(55,100)+\n  ylim(55,100)+\n  xlab('Midyear Evaluation')+\n  ylab('Year-End Evaluation')+\n  theme_bw()"},{"path":"ch1.html","id":"example-2","chapter":"1 Linear Regression with One Predictor Variable","heading":"1.1.2 Example 2","text":"","code":"\nfig1.3 <- read.csv(here('/data/fig1.3.csv/'),header=FALSE)\n\nggplot(fig1.3,aes(x=V1,y=V2))+\n  geom_point(size=4,col='gray42')+\n  geom_smooth(method=lm,formula = y ~ x + I(x^2), se=FALSE,lty=2,col='black')+\n  xlim(7.5,25)+\n  ylim(1,30)+\n  xlab('Age')+\n  ylab('Stereoid Level')+\n  theme_bw()"},{"path":"ch1.html","id":"regression-model-an-their-uses","chapter":"1 Linear Regression with One Predictor Variable","heading":"1.2 Regression Model an Their Uses","text":"","code":""},{"path":"ch1.html","id":"simple-linear-regression-model-with-distributions-of-error-terms-unspecified","chapter":"1 Linear Regression with One Predictor Variable","heading":"1.3 Simple Linear Regression Model with Distributions of Error Terms Unspecified","text":"","code":""},{"path":"ch1.html","id":"example-page-10","chapter":"1 Linear Regression with One Predictor Variable","heading":"1.3.1 Example (page 10)","text":"\\[Y_i = 9.5 + 2.1X_i + \\epsilon_i\\]","code":"\nb0 = 9.5\nb1 = 2.1\n        \nx = 0:70\n        \nmean = b0 + b1*x\n    \nerr.sd = 5\n        \n  plot(x,mean,type=\"l\",\n       ylim=c(40,120),\n       xlim=c(10,50),\n       cex=1,\n       pch=19,\n       xlab=\"Number of Bids Prepared\",\n       ylab=\"Hours\",\n       xaxt='n')\n        \n  axis(side=1,at=c(0,25,45))\n  abline(b0,b1)\n        \n  dens = dnorm(seq(-3,3,.01),0,1)\n\n  for(i in c(26,46)){\n            \n    x. = x[i] - 5*dens\n    y. = mean[i]+seq(-3,3,.01)*err.sd\n    points(x.,y.,type=\"l\",lty=2)\n    abline(v=x[i],lty=2,col=\"gray\")\n  }"},{"path":"ch1.html","id":"data-for-regression-analysis","chapter":"1 Linear Regression with One Predictor Variable","heading":"1.4 Data for Regression Analysis","text":"","code":""},{"path":"ch1.html","id":"overview-of-steps-in-regression-analysis","chapter":"1 Linear Regression with One Predictor Variable","heading":"1.5 Overview of Steps in Regression Analysis","text":"","code":""},{"path":"ch1.html","id":"estimation-of-regression-function","chapter":"1 Linear Regression with One Predictor Variable","heading":"1.6 Estimation of Regression Function","text":"","code":""},{"path":"ch1.html","id":"example-page-15","chapter":"1 Linear Regression with One Predictor Variable","heading":"1.6.1 Example (page 15)","text":"","code":"\nAge      <- c(20,55,30)\nAttempts <- c(5, 12, 10)\n\nfig1.9 <- data.frame(Age=Age,Attempts=Attempts)\n\nggplot(data= fig1.9, aes(x=Age,y=Attempts))+\n    geom_point(size=4,color='gray42')+\n    xlim(c(10,60))+\n    ylim(c(0,15))+\n  theme_bw()+\n    geom_hline(yintercept = 9)+\n    geom_segment(x = fig1.9[1,1], y = fig1.9[1,2], xend = fig1.9[1,1], yend = 9)+\n  geom_segment(x = fig1.9[2,1], y = fig1.9[2,2], xend = fig1.9[2,1], yend = 9)+\n  geom_segment(x = fig1.9[3,1], y = fig1.9[3,2], xend = fig1.9[3,1], yend = 9)+\n    xlab('Age')+\n    ylab('Attempts')\n# Sum of Squared Deviations \n\nsum((Attempts - 9)^2)[1] 26\nmod <- lm(Attempts ~ 1 + Age,d=fig1.9)\n\nggplot(data= fig1.9, aes(x=Age,y=Attempts))+\n    geom_point(size=4,color='gray42')+\n  geom_abline(intercept = coef(mod)[1],slope=coef(mod)[2])+\n    xlim(c(10,60))+\n    ylim(c(0,15))+\n  theme_bw()+\n    geom_segment(x = fig1.9[1,1], y = fig1.9[1,2], xend = fig1.9[1,1], \n                 yend = predict(mod, newdata = data.frame(Age=c(fig1.9[1,1]))))+\n    geom_segment(x = fig1.9[2,1], y = fig1.9[2,2], xend = fig1.9[2,1], \n                 yend = predict(mod, newdata = data.frame(Age=c(fig1.9[2,1]))))+\n    geom_segment(x = fig1.9[3,1], y = fig1.9[3,2], xend = fig1.9[3,1], \n                 yend = predict(mod, newdata = data.frame(Age=c(fig1.9[3,1]))))+\n    xlab('Age')+\n    ylab('Attempts')\n# Model predictions\n\npredict(mod)     1      2      3 \r\n 6.346 12.538  8.115 \n# Model Residuals\n\nresid(mod)      1       2       3 \r\n-1.3462 -0.5385  1.8846 \n# Sum of Squared Deviations \n\nsum((Attempts - predict(mod))^2)[1] 5.654\nsum(resid(mod)^2)[1] 5.654"},{"path":"ch1.html","id":"example-page-19","chapter":"1 Linear Regression with One Predictor Variable","heading":"1.6.2 Example (page 19)","text":"","code":"\ntable1.1 <- read.table(here('/data/CH01TA01.txt/'),header=FALSE)\n\ncolnames(table1.1) <- c('lot.size','work.hours')\n\ntable1.1   lot.size work.hours\r\n1        80        399\r\n2        30        121\r\n3        50        221\r\n4        90        376\r\n5        70        361\r\n6        60        224\r\n7       120        546\r\n8        80        352\r\n9       100        353\r\n10       50        157\r\n11       40        160\r\n12       70        252\r\n13       90        389\r\n14       20        113\r\n15      110        435\r\n16      100        420\r\n17       30        212\r\n18       50        268\r\n19       90        377\r\n20      110        421\r\n21       30        273\r\n22       90        468\r\n23       40        244\r\n24       80        342\r\n25       70        323\nggplot(table1.1,aes(x=lot.size,y=work.hours))+\n  geom_point(size=3,col='gray42')+\n  geom_smooth(method=lm,se=FALSE,lty=2,lwd=0.5,col='black')+\n  xlim(0,150)+\n  ylim(0,600)+\n  xlab('Lot Size')+\n  ylab('Hours')+\n  theme_bw()\nrequire(lm.beta)\nrequire(car)\n\nmod <- lm(work.hours ~ 1 + lot.size,d=table1.1)\n\nAnova(mod,type=3)Anova Table (Type III tests)\r\n\r\nResponse: work.hours\r\n            Sum Sq Df F value        Pr(>F)    \r\n(Intercept)  13530  1    5.68         0.026 *  \r\nlot.size    252378  1  105.88 0.00000000044 ***\r\nResiduals    54825 23                          \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nsummary(mod)\r\nCall:\r\nlm(formula = work.hours ~ 1 + lot.size, data = table1.1)\r\n\r\nResiduals:\r\n   Min     1Q Median     3Q    Max \r\n-83.88 -34.09  -5.98  38.83 103.53 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value      Pr(>|t|)    \r\n(Intercept)   62.366     26.177    2.38         0.026 *  \r\nlot.size       3.570      0.347   10.29 0.00000000044 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 48.8 on 23 degrees of freedom\r\nMultiple R-squared:  0.822, Adjusted R-squared:  0.814 \r\nF-statistic:  106 on 1 and 23 DF,  p-value: 0.000000000445\ncoef(mod)(Intercept)    lot.size \r\n      62.37        3.57 \ntable1.1$x_xbar <- table1.1$lot.size - mean(table1.1$lot.size)\ntable1.1$y_ybar <- table1.1$work.hours - mean(table1.1$work.hours)\ntable1.1$x_xbar_y_ybar <- table1.1$x_xbar*table1.1$y_ybar\ntable1.1$x_xbar.sq <- table1.1$x_xbar^2\ntable1.1$y_ybar.sq <- table1.1$y_ybar^2\n\ntable1.1   lot.size work.hours x_xbar  y_ybar x_xbar_y_ybar x_xbar.sq y_ybar.sq\r\n1        80        399     10   86.72         867.2       100    7520.4\r\n2        30        121    -40 -191.28        7651.2      1600   36588.0\r\n3        50        221    -20  -91.28        1825.6       400    8332.0\r\n4        90        376     20   63.72        1274.4       400    4060.2\r\n5        70        361      0   48.72           0.0         0    2373.6\r\n6        60        224    -10  -88.28         882.8       100    7793.4\r\n7       120        546     50  233.72       11686.0      2500   54625.0\r\n8        80        352     10   39.72         397.2       100    1577.7\r\n9       100        353     30   40.72        1221.6       900    1658.1\r\n10       50        157    -20 -155.28        3105.6       400   24111.9\r\n11       40        160    -30 -152.28        4568.4       900   23189.2\r\n12       70        252      0  -60.28           0.0         0    3633.7\r\n13       90        389     20   76.72        1534.4       400    5886.0\r\n14       20        113    -50 -199.28        9964.0      2500   39712.5\r\n15      110        435     40  122.72        4908.8      1600   15060.2\r\n16      100        420     30  107.72        3231.6       900   11603.6\r\n17       30        212    -40 -100.28        4011.2      1600   10056.1\r\n18       50        268    -20  -44.28         885.6       400    1960.7\r\n19       90        377     20   64.72        1294.4       400    4188.7\r\n20      110        421     40  108.72        4348.8      1600   11820.0\r\n21       30        273    -40  -39.28        1571.2      1600    1542.9\r\n22       90        468     20  155.72        3114.4       400   24248.7\r\n23       40        244    -30  -68.28        2048.4       900    4662.2\r\n24       80        342     10   29.72         297.2       100     883.3\r\n25       70        323      0   10.72           0.0         0     114.9\nb1 = sum(table1.1$x_xbar_y_ybar)/sum(table1.1$x_xbar.sq)\nb1[1] 3.57\nb0 = mean(table1.1$work.hours) - b1*mean(table1.1$lot.size)\nb0[1] 62.37"},{"path":"ch1.html","id":"example-page-21","chapter":"1 Linear Regression with One Predictor Variable","heading":"1.6.3 Example (page 21)","text":"\\[ \\hat{Y} = 62.37 + 35702*X\\]","code":"\npredict(mod, newdata = data.frame(lot.size=65))    1 \r\n294.4 "},{"path":"ch1.html","id":"table-1.2-page-22","chapter":"1 Linear Regression with One Predictor Variable","heading":"1.6.4 Table 1.2 (page 22)","text":"","code":"\ntable1.1 <- read.table(here('/data/CH01TA01.txt/'),header=FALSE)\ncolnames(table1.1) <- c('lot.size','work.hours')\n\nmod <- lm(work.hours ~ 1 + lot.size,d=table1.1)\n\ntable1.1$predicted <- predict(mod)\ntable1.1$residuals <- resid(mod)\ntable1.1$residuals.squared <- resid(mod)^2\n\ntable1.1   lot.size work.hours predicted residuals residuals.squared\r\n1        80        399     348.0   51.0180         2602.8343\r\n2        30        121     169.5  -48.4719         2349.5270\r\n3        50        221     240.9  -19.8760          395.0538\r\n4        90        376     383.7   -7.6840           59.0445\r\n5        70        361     312.3   48.7200         2373.6384\r\n6        60        224     276.6  -52.5780         2764.4440\r\n7       120        546     490.8   55.2099         3048.1329\r\n8        80        352     348.0    4.0180           16.1442\r\n9       100        353     419.4  -66.3861         4407.1090\r\n10       50        157     240.9  -83.8760         7035.1766\r\n11       40        160     205.2  -45.1739         2040.6848\r\n12       70        252     312.3  -60.2800         3633.6784\r\n13       90        389     383.7    5.3160           28.2594\r\n14       20        113     133.8  -20.7699          431.3887\r\n15      110        435     455.1  -20.0881          403.5310\r\n16      100        420     419.4    0.6139            0.3769\r\n17       30        212     169.5   42.5281         1808.6377\r\n18       50        268     240.9   27.1240          735.7136\r\n19       90        377     383.7   -6.6840           44.6764\r\n20      110        421     455.1  -34.0881         1161.9973\r\n21       30        273     169.5  103.5281        10718.0635\r\n22       90        468     383.7   84.3160         7109.1810\r\n23       40        244     205.2   38.8261         1507.4630\r\n24       80        342     348.0   -5.9820           35.7846\r\n25       70        323     312.3   10.7200          114.9184"},{"path":"ch1.html","id":"alternative-model-with-mean-centering-page-22","chapter":"1 Linear Regression with One Predictor Variable","heading":"1.6.5 Alternative Model with Mean Centering (page 22)","text":"","code":"\ntable1.1 <- read.table(here('/data/CH01TA01.txt/'),header=FALSE)\ncolnames(table1.1) <- c('lot.size','work.hours')\n\ntable1.1$lot.size_centered <- table1.1$lot.size - mean(table1.1$lot.size)\n\nmod <- lm(work.hours ~ 1 + lot.size_centered,d=table1.1)\n\nAnova(mod,type=3)Anova Table (Type III tests)\r\n\r\nResponse: work.hours\r\n                   Sum Sq Df F value               Pr(>F)    \r\n(Intercept)       2437970  1    1023 < 0.0000000000000002 ***\r\nlot.size_centered  252378  1     106        0.00000000044 ***\r\nResiduals           54825 23                                 \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nsummary(mod)\r\nCall:\r\nlm(formula = work.hours ~ 1 + lot.size_centered, data = table1.1)\r\n\r\nResiduals:\r\n   Min     1Q Median     3Q    Max \r\n-83.88 -34.09  -5.98  38.83 103.53 \r\n\r\nCoefficients:\r\n                  Estimate Std. Error t value             Pr(>|t|)    \r\n(Intercept)        312.280      9.765    32.0 < 0.0000000000000002 ***\r\nlot.size_centered    3.570      0.347    10.3        0.00000000044 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 48.8 on 23 degrees of freedom\r\nMultiple R-squared:  0.822, Adjusted R-squared:  0.814 \r\nF-statistic:  106 on 1 and 23 DF,  p-value: 0.000000000445\ncoef(mod)      (Intercept) lot.size_centered \r\n           312.28              3.57 "},{"path":"ch1.html","id":"estimation-of-error-terms-variance","chapter":"1 Linear Regression with One Predictor Variable","heading":"1.7 Estimation of Error Terms Variance","text":"","code":"\ntable1.1 <- read.table(here('/data/CH01TA01.txt/'),header=FALSE)\ncolnames(table1.1) <- c('lot.size','work.hours')\n\nmod <- lm(work.hours ~ 1 + lot.size,d=table1.1)\n\ntable1.1$predicted <- predict(mod)\ntable1.1$residuals <- resid(mod)\ntable1.1$residuals.squared <- resid(mod)^2\n\nsse = sum(table1.1$residuals.squared)\nsse[1] 54825\nmse = sum(table1.1$residuals.squared)/(nrow(table1.1)-2)\nmse [1] 2384\nsqrt(mse)[1] 48.82"},{"path":"ch1.html","id":"normal-error-regression-model","chapter":"1 Linear Regression with One Predictor Variable","heading":"1.8 Normal Error Regression Model","text":"","code":""},{"path":"ch1.html","id":"least-square-estimation","chapter":"1 Linear Regression with One Predictor Variable","heading":"1.8.1 Least Square Estimation","text":"","code":"\ntable1.1 <- read.table(here('/data/CH01TA01.txt/'),header=FALSE)\ncolnames(table1.1) <- c('lot.size','work.hours')\n\nbeta0 <- seq(50,75,.1)\nbeta1 <- seq(1,6,.01)\n\nols <- expand.grid(beta0,beta1)\ncolnames(ols) <- c('beta0','beta1')\nols$ssr <- NA\n\nfor(i in 1:nrow(ols)){\n    p = ols[i,1] + ols[i,2]*table1.1$lot.size\n    ols[i,3] = sum((table1.1$work.hours - p)^2)\n}\n\nrequire(lattice)\n\nwireframe(ssr ~ beta0 * beta1,\n          data = ols,\n          shade=TRUE,\n          screen = list(z = 40, x = -60, y=0),\n        scales = list(arrows=FALSE),\n        xlab = expression(beta[0]), \n        ylab = expression(beta[1]), \n        zlab = \"SSR\")\n\nols[which.min(ols$ssr),]      beta0 beta1   ssr\r\n64632  62.4  3.57 54825"},{"path":"ch1.html","id":"maximum-likelihood-estimation","chapter":"1 Linear Regression with One Predictor Variable","heading":"1.8.2 Maximum Likelihood Estimation","text":"","code":"\ntable1.1 <- read.table(here('/data/CH01TA01.txt/'),header=FALSE)\ncolnames(table1.1) <- c('lot.size','work.hours')\n\nbeta0 <- seq(50,75,.1)\nbeta1 <- seq(1,6,.01)\n\nmle <- expand.grid(beta0,beta1)\ncolnames(mle) <- c('beta0','beta1')\nmle$loglikelihood <- NA\n\nmse = 2383 # assumed to be known\n\nfor(i in 1:nrow(mle)){\n    p = ols[i,1] + ols[i,2]*table1.1$lot.size\n    mle[i,3] =  sum(log(dnorm((table1.1$work.hours - p)/sqrt(mse))))\n}\n\nwireframe(loglikelihood ~ beta0 * beta1,\n          data = mle,\n          shade=TRUE,\n          screen = list(z = 40, x = -60, y=0),\n          scales = list(arrows=FALSE),\n          xlab = expression(beta[0]), \n          ylab = expression(beta[1]), \n          zlab = \"Loglikelihood\")\n\nmle[which.max(mle$loglikelihood),]      beta0 beta1 loglikelihood\r\n64632  62.4  3.57        -34.48"},{"path":"ch1.html","id":"problems","chapter":"1 Linear Regression with One Predictor Variable","heading":"1.9 Problems","text":"","code":""},{"path":"chapter-2.html","id":"chapter-2","chapter":"Chapter 2","heading":"Chapter 2","text":"","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"Kutner, Michael H, Christopher J Nachtsheim, John Neter, William Li, others. 2005. Applied Linear Statistical Models. Vol. 5. McGraw-Hill Irwin New York.","code":""}]
